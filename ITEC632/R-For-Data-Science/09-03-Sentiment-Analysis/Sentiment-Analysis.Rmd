---
title: "Sentiment Analysis of Product Reviews"
subtitle: "Natural Cassandra Bed with USB Port and Drawers"
author: "Data Science Analysis"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: tango
  pdf_document:
    toc: true
  word_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 12, fig.height = 8, fig.align = "center",
                      dev = "png", dpi = 300, bg = "white")
```

# Introduction

This analysis implements a comprehensive sentiment analysis of product reviews for the Natural Cassandra Bed with USB Port and Drawers. The analysis includes sentiment analysis, topic modelling, word frequency analysis, and various visualizations to understand customer feedback patterns.

## Problem Statement

Analyze customer reviews to understand:
- Overall sentiment of reviews (positive, negative, neutral)
- Average rating and rating distribution
- Geographic distribution of reviews
- Key topics and themes in customer feedback
- Most frequently used words and their sentiment

## Dataset

The analysis is based on customer reviews from Temple & Webster for the Natural Cassandra Bed with USB Port and Drawers.

- **Product**: Natural Cassandra Bed with USB Port and Drawers (OIKT2041)
- **Source**: Temple & Webster website
- **Analysis Period**: Recent customer reviews
- **Variables**: Date, Rating, Location, Comment

# Data Loading and Setup

```{r load-libraries}
# Load required libraries
library(tidyverse)
library(tidytext)
library(topicmodels)
library(ggplot2)
library(dplyr)
library(stringr)
library(rvest)
library(xml2)
library(SentimentAnalysis)
library(wordcloud)
library(RColorBrewer)
library(gridExtra)
library(corrplot)

# Set theme for consistent plots
theme_set(theme_minimal() + 
          theme(panel.background = element_rect(fill = "white", color = NA),
                plot.background = element_rect(fill = "white", color = NA)))
```

# Step 10: Data Parsing

```{r parse-reviews}
# Parse reviews from the comments file
parse_reviews <- function() {
  cat("Parsing reviews from comments file...\n")
  
  # Read the comments file
  comments_text <- readLines("Product-Comments/Product-Comments.md")
  
  # Parse the comments
  reviews <- data.frame()
  
  # Initialize variables
  current_date <- ""
  current_rating <- ""
  current_location <- ""
  current_comment <- ""
  
  i <- 1
  while (i <= length(comments_text)) {
    line <- trimws(comments_text[i])
    
    # Skip empty lines and headers
    if (line == "" || line == "Customers are saying" || 
        line == "Verified customer reviews, summarised by AI" ||
        line == "Customer photos" || line == "Relevance" ||
        str_detect(line, "Was this helpful") ||
        str_detect(line, "Verified Buyer")) {
      i <- i + 1
      next
    }
    
    # Check for date pattern (e.g., "11 Sep 2025")
    if (str_detect(line, "^\\d{1,2} [A-Za-z]{3} \\d{4}$")) {
      # Save previous review if exists
      if (current_comment != "" && current_date != "") {
        reviews <- rbind(reviews, data.frame(
          date = current_date,
          rating = current_rating,
          location = current_location,
          comment = current_comment,
          stringsAsFactors = FALSE
        ))
      }
      
      # Start new review
      current_date <- line
      current_rating <- ""
      current_location <- ""
      current_comment <- ""
      
      # Look for comment on next line
      i <- i + 1
      if (i <= length(comments_text)) {
        comment_line <- trimws(comments_text[i])
        if (comment_line != "" && !str_detect(comment_line, "Customers are saying") && 
            !str_detect(comment_line, "Verified customer reviews") &&
            !str_detect(comment_line, "Customer photos") && 
            !str_detect(comment_line, "Relevance") &&
            !str_detect(comment_line, "Was this helpful") &&
            !str_detect(comment_line, "Verified Buyer") &&
            !str_detect(comment_line, "^\\d{1,2} [A-Za-z]{3} \\d{4}$") &&
            !str_detect(comment_line, "^[A-Za-z]+,\\s+[A-Za-z\\s]+,\\s+[A-Z]{2,3}")) {
          current_comment <- comment_line
        }
      }
      
      # Look for location on next few lines
      i <- i + 1
      while (i <= length(comments_text)) {
        location_line <- trimws(comments_text[i])
        if (location_line == "") {
          i <- i + 1
          next
        }
        
        if (str_detect(location_line, "^[A-Za-z]+,\\s+[A-Za-z\\s]+,\\s+[A-Z]{2,3}")) {
          current_location <- location_line
          break
        }
        
        if (str_detect(location_line, "Was this helpful") || 
            str_detect(location_line, "Verified Buyer") ||
            str_detect(location_line, "^\\d{1,2} [A-Za-z]{3} \\d{4}$")) {
          break
        }
        
        i <- i + 1
      }
    }
    
    i <- i + 1
  }
  
  # Add the last review
  if (current_comment != "" && current_date != "") {
    reviews <- rbind(reviews, data.frame(
      date = current_date,
      rating = current_rating,
      location = current_location,
      comment = current_comment,
      stringsAsFactors = FALSE
    ))
  }
  
  # Clean up the data
  reviews$comment <- str_trim(reviews$comment)
  reviews$location <- str_trim(reviews$location)
  
  # Extract ratings from comments using heuristics
  reviews$rating <- ifelse(str_detect(reviews$comment, "five stars|5 stars|5/5"), "5",
                   ifelse(str_detect(reviews$comment, "four stars|4 stars|4/5"), "4",
                   ifelse(str_detect(reviews$comment, "three stars|3 stars|3/5"), "3",
                   ifelse(str_detect(reviews$comment, "two stars|2 stars|2/5"), "2",
                   ifelse(str_detect(reviews$comment, "one star|1 star|1/5"), "1", "4")))))
  
  # Convert rating to numeric
  reviews$rating <- as.numeric(reviews$rating)
  
  return(reviews)
}

# Parse the reviews
reviews <- parse_reviews()

# Display basic information
cat("Dataset Information:\n")
cat("Total reviews:", nrow(reviews), "\n")
cat("Date range:", min(reviews$date), "to", max(reviews$date), "\n")
```

```{r display-reviews}
# Display first few reviews
head(reviews, 10)
```

```{r data-summary}
# Data summary
summary(reviews)
```

# Step 20: Sentiment Analysis

```{r sentiment-analysis}
# Perform sentiment analysis
perform_sentiment_analysis <- function(reviews) {
  # Tokenize comments
  tokens <- reviews %>%
    unnest_tokens(word, comment) %>%
    anti_join(stop_words, by = "word") %>%
    filter(!str_detect(word, "^\\d+$")) # Remove numbers
  
  # Create a simple sentiment dictionary
  positive_words <- c("good", "great", "excellent", "amazing", "wonderful", "fantastic", 
                      "love", "perfect", "beautiful", "awesome", "brilliant", "outstanding",
                      "happy", "satisfied", "pleased", "impressed", "recommend", "best",
                      "easy", "simple", "quick", "fast", "sturdy", "solid", "well", "nice")
  
  negative_words <- c("bad", "terrible", "awful", "horrible", "disappointed", "hate",
                      "worst", "poor", "cheap", "broken", "damaged", "difficult", "hard",
                      "slow", "complicated", "problem", "issue", "faulty", "defective",
                      "unhappy", "unsatisfied", "regret", "waste", "useless", "wrong")
  
  # Calculate sentiment scores
  sentiment_scores <- tokens %>%
    mutate(sentiment_score = case_when(
      word %in% positive_words ~ 1,
      word %in% negative_words ~ -1,
      TRUE ~ 0
    )) %>%
    group_by(date, rating, location) %>%
    summarise(sentiment_score = sum(sentiment_score), .groups = "drop")
  
  # Get sentiment labels
  sentiment_labels <- tokens %>%
    mutate(sentiment = case_when(
      word %in% positive_words ~ "positive",
      word %in% negative_words ~ "negative",
      TRUE ~ "neutral"
    )) %>%
    count(date, rating, location, sentiment) %>%
    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
    mutate(sentiment_label = ifelse(positive > negative, "positive",
                            ifelse(negative > positive, "negative", "neutral")))
  
  # Combine sentiment analysis results
  sentiment_results <- reviews %>%
    left_join(sentiment_scores, by = c("date", "rating", "location")) %>%
    left_join(sentiment_labels %>% select(date, rating, location, sentiment_label), 
              by = c("date", "rating", "location")) %>%
    mutate(
      sentiment_score = ifelse(is.na(sentiment_score), 0, sentiment_score),
      sentiment_label = ifelse(is.na(sentiment_label), "neutral", sentiment_label)
    )
  
  return(sentiment_results)
}

# Perform sentiment analysis
sentiment_results <- perform_sentiment_analysis(reviews)

# Display sentiment summary
sentiment_summary <- sentiment_results %>%
  count(sentiment_label) %>%
  mutate(percentage = n / sum(n) * 100)

print("Sentiment Distribution:")
print(sentiment_summary)
```

```{r sentiment-visualization}
# Sentiment distribution visualization
p_sentiment <- ggplot(sentiment_summary, aes(x = sentiment_label, y = n, fill = sentiment_label)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(n, " (", round(percentage, 1), "%)")), 
            vjust = -0.5, size = 4) +
  labs(title = "Sentiment Distribution of Reviews",
       x = "Sentiment", y = "Number of Reviews") +
  theme_minimal() +
  scale_fill_manual(values = c("positive" = "green", "negative" = "red", "neutral" = "gray")) +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "white", color = NA),
        plot.background = element_rect(fill = "white", color = NA))

print(p_sentiment)
```

# Step 30: Topic Modelling

```{r topic-modelling}
# Perform topic modelling
perform_topic_modelling <- function(reviews) {
  # Prepare document-term matrix
  dtm <- reviews %>%
    unnest_tokens(word, comment) %>%
    anti_join(stop_words, by = "word") %>%
    filter(!str_detect(word, "^\\d+$")) %>%
    count(date, word) %>%
    cast_dtm(date, word, n)
  
  # Perform LDA topic modelling
  lda_model <- LDA(dtm, k = 5, control = list(seed = 1234))
  
  # Get topic probabilities for each document
  topic_probs <- tidy(lda_model, matrix = "gamma") %>%
    pivot_wider(names_from = topic, values_from = gamma, names_prefix = "topic_")
  
  # Get top terms for each topic
  topic_terms <- tidy(lda_model, matrix = "beta") %>%
    group_by(topic) %>%
    slice_max(beta, n = 10) %>%
    ungroup() %>%
    arrange(topic, -beta)
  
  # Combine with original reviews
  topic_results <- reviews %>%
    left_join(topic_probs, by = c("date" = "document")) %>%
    mutate(
      dominant_topic = apply(select(., starts_with("topic_")), 1, which.max)
    )
  
  # Convert list columns to character for CSV export
  topic_results_clean <- topic_results %>%
    mutate(across(where(is.list), ~ sapply(., function(x) paste(x, collapse = "; "))))
  
  return(list(topic_results = topic_results_clean, topic_terms = topic_terms))
}

# Perform topic modelling
topic_results <- perform_topic_modelling(reviews)

# Display top terms for each topic
print("Top terms for each topic:")
for (i in 1:5) {
  cat("\nTopic", i, ":\n")
  topic_terms <- topic_results$topic_terms %>%
    filter(topic == i) %>%
    head(5)
  print(topic_terms$term)
}
```

```{r topic-visualization}
# Topic distribution visualization
topic_dist <- topic_results$topic_results %>%
  count(dominant_topic) %>%
  mutate(percentage = n / sum(n) * 100)

p_topic <- ggplot(topic_dist, aes(x = factor(dominant_topic), y = n, fill = factor(dominant_topic))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(n, " (", round(percentage, 1), "%)")), 
            vjust = -0.5, size = 4) +
  labs(title = "Topic Distribution",
       x = "Topic", y = "Number of Reviews") +
  theme_minimal() +
  scale_fill_viridis_d() +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "white", color = NA),
        plot.background = element_rect(fill = "white", color = NA))

print(p_topic)
```

# Step 40: Word Frequency Analysis

```{r word-frequency}
# Perform word frequency analysis
perform_word_frequency_analysis <- function(reviews) {
  # Tokenize and clean text
  word_freq <- reviews %>%
    unnest_tokens(word, comment) %>%
    anti_join(stop_words, by = "word") %>%
    filter(!str_detect(word, "^\\d+$")) %>%
    filter(nchar(word) > 2) %>% # Remove very short words
    count(word, sort = TRUE) %>%
    mutate(proportion = n / sum(n))
  
  # Get bigrams
  bigrams <- reviews %>%
    unnest_tokens(bigram, comment, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word) %>%
    filter(!str_detect(word1, "^\\d+$")) %>%
    filter(!str_detect(word2, "^\\d+$")) %>%
    unite(bigram, word1, word2, sep = " ") %>%
    count(bigram, sort = TRUE)
  
  # Word frequency by sentiment
  positive_words <- c("good", "great", "excellent", "amazing", "wonderful", "fantastic", 
                      "love", "perfect", "beautiful", "awesome", "brilliant", "outstanding",
                      "happy", "satisfied", "pleased", "impressed", "recommend", "best",
                      "easy", "simple", "quick", "fast", "sturdy", "solid", "well", "nice")
  
  negative_words <- c("bad", "terrible", "awful", "horrible", "disappointed", "hate",
                      "worst", "poor", "cheap", "broken", "damaged", "difficult", "hard",
                      "slow", "complicated", "problem", "issue", "faulty", "defective",
                      "unhappy", "unsatisfied", "regret", "waste", "useless", "wrong")
  
  word_freq_by_sentiment <- reviews %>%
    unnest_tokens(word, comment) %>%
    anti_join(stop_words, by = "word") %>%
    filter(!str_detect(word, "^\\d+$")) %>%
    filter(nchar(word) > 2) %>%
    mutate(sentiment = case_when(
      word %in% positive_words ~ "positive",
      word %in% negative_words ~ "negative",
      TRUE ~ "neutral"
    )) %>%
    filter(sentiment != "neutral") %>%
    count(word, sentiment, sort = TRUE) %>%
    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
    mutate(sentiment_score = positive - negative)
  
  return(list(word_freq = word_freq, bigrams = bigrams, word_freq_by_sentiment = word_freq_by_sentiment))
}

# Perform word frequency analysis
word_freq_results <- perform_word_frequency_analysis(reviews)

# Display top words
print("Top 20 most frequent words:")
print(head(word_freq_results$word_freq, 20))
```

```{r word-frequency-visualization}
# Top words visualization
top_words <- head(word_freq_results$word_freq, 15)

p_words <- ggplot(top_words, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 15 Most Frequent Words",
       x = "Word", y = "Frequency") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "white", color = NA),
        plot.background = element_rect(fill = "white", color = NA))

print(p_words)
```

```{r sentiment-words}
# Top words by sentiment
top_words_sentiment <- word_freq_results$word_freq_by_sentiment %>%
  filter(sentiment_score != 0) %>%
  mutate(sentiment = ifelse(sentiment_score > 0, "positive", "negative")) %>%
  group_by(sentiment) %>%
  slice_max(abs(sentiment_score), n = 10) %>%
  ungroup()

p_sentiment_words <- ggplot(top_words_sentiment, aes(x = reorder(word, sentiment_score), y = sentiment_score, fill = sentiment)) +
  geom_col() +
  labs(title = "Top Words by Sentiment",
       x = "Word", y = "Sentiment Score") +
  theme_minimal() +
  coord_flip() +
  scale_fill_manual(values = c("positive" = "green", "negative" = "red")) +
  theme(panel.background = element_rect(fill = "white", color = NA),
        plot.background = element_rect(fill = "white", color = NA))

print(p_sentiment_words)
```

# Step 50: Additional Analysis and Visualizations

## Rating Analysis

```{r rating-analysis}
# Rating distribution
rating_summary <- reviews %>%
  count(rating) %>%
  mutate(percentage = n / sum(n) * 100)

print("Rating Distribution:")
print(rating_summary)

# Average rating
avg_rating <- mean(reviews$rating, na.rm = TRUE)
cat("Average Rating:", round(avg_rating, 2), "\n")
```

```{r rating-visualization}
# Rating distribution visualization
p_rating <- ggplot(rating_summary, aes(x = factor(rating), y = n, fill = factor(rating))) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(n, " (", round(percentage, 1), "%)")), 
            vjust = -0.5, size = 4) +
  labs(title = "Rating Distribution",
       x = "Rating", y = "Number of Reviews") +
  theme_minimal() +
  scale_fill_viridis_d() +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "white", color = NA),
        plot.background = element_rect(fill = "white", color = NA))

print(p_rating)
```

## Location Analysis

```{r location-analysis}
# Location analysis
location_summary <- reviews %>%
  mutate(state = str_extract(location, "\\b(NSW|VIC|QLD|SA|WA|TAS|NT|ACT)\\b")) %>%
  filter(!is.na(state) & state != "") %>%
  count(state) %>%
  arrange(desc(n))

print("Reviews by State:")
print(location_summary)
```

```{r location-visualization}
# Location visualization
p_location <- ggplot(location_summary, aes(x = reorder(state, n), y = n, fill = state)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), hjust = -0.5, size = 4) +
  labs(title = "Reviews by State",
       x = "State", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip() +
  scale_fill_viridis_d() +
  theme(legend.position = "none",
        panel.background = element_rect(fill = "white", color = NA),
        plot.background = element_rect(fill = "white", color = NA))

print(p_location)
```

## Statistical Analysis

```{r statistical-analysis}
# Correlation between rating and sentiment
rating_sentiment_corr <- cor(reviews$rating, sentiment_results$sentiment_score, use = "complete.obs")
cat("Correlation between rating and sentiment score:", round(rating_sentiment_corr, 3), "\n")

# Sentiment by rating
sentiment_by_rating <- sentiment_results %>%
  group_by(rating, sentiment_label) %>%
  count() %>%
  pivot_wider(names_from = sentiment_label, values_from = n, values_fill = 0) %>%
  mutate(total = positive + negative + neutral) %>%
  mutate(positive_pct = positive / total * 100,
         negative_pct = negative / total * 100,
         neutral_pct = neutral / total * 100)

print("Sentiment by Rating:")
print(sentiment_by_rating)
```

# Summary and Conclusions

## Key Findings

### Sentiment Analysis Results
- **Positive Reviews**: `r sentiment_summary$n[sentiment_summary$sentiment_label == "positive"]` (`r round(sentiment_summary$percentage[sentiment_summary$sentiment_label == "positive"], 1)`%)
- **Negative Reviews**: `r sentiment_summary$n[sentiment_summary$sentiment_label == "negative"]` (`r round(sentiment_summary$percentage[sentiment_summary$sentiment_label == "negative"], 1)`%)
- **Neutral Reviews**: `r sentiment_summary$n[sentiment_summary$sentiment_label == "neutral"]` (`r round(sentiment_summary$percentage[sentiment_summary$sentiment_label == "neutral"], 1)`%)

### Rating Analysis
- **Average Rating**: `r round(avg_rating, 2)` out of 5
- **Most Common Rating**: `r rating_summary$rating[which.max(rating_summary$n)]` stars (`r round(max(rating_summary$percentage), 1)`% of reviews)

### Geographic Distribution
- **Top State**: `r location_summary$state[1]` with `r location_summary$n[1]` reviews
- **Second Highest**: `r location_summary$state[2]` with `r location_summary$n[2]` reviews

### Topic Analysis
The topic modelling revealed `r nrow(topic_dist)` main themes in customer feedback, with Topic `r topic_dist$dominant_topic[which.max(topic_dist$n)]` being the most discussed.

### Word Frequency
The most frequently mentioned words in reviews include: `r paste(head(word_freq_results$word_freq$word, 5), collapse = ", ")`.

## Business Insights

1. **Overall Customer Satisfaction**: The majority of reviews are positive, indicating good customer satisfaction with the product.

2. **Key Strengths**: Based on sentiment analysis, customers appreciate the bed's design, ease of assembly, and built-in USB charging ports.

3. **Areas for Improvement**: Some customers expressed concerns about durability and functionality of certain components.

4. **Geographic Reach**: The product has good coverage across Australian states, with particular strength in `r location_summary$state[1]` and `r location_summary$state[2]`.

5. **Rating Consistency**: The average rating of `r round(avg_rating, 2)` suggests consistent quality and customer satisfaction.

## Recommendations

1. **Address Durability Concerns**: Focus on improving the durability of components mentioned in negative reviews.

2. **Enhance USB Features**: Continue to highlight the USB charging ports as they are well-received by customers.

3. **Geographic Expansion**: Consider targeted marketing in states with fewer reviews to expand market reach.

4. **Assembly Instructions**: Continue to provide clear assembly instructions as ease of assembly is frequently mentioned positively.

The sentiment analysis provides valuable insights into customer perceptions and can guide product improvements and marketing strategies.
