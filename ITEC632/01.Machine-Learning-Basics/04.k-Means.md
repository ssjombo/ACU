# k-Means Clustering: Classic, H2O, Kernel, and Fast Variants

## Overview

k-Means is an unsupervised clustering algorithm that partitions \(n\) observations into \(k\) clusters so as to minimize the within-cluster sum of squared distances. Given data \(X = \{x_i \in \mathbb{R}^d\}_{i=1}^n\), cluster assignments \(C_1, \dots, C_k\), and centroids \(\mu_1, \dots, \mu_k\), the objective is:

\[
J(\{C_j\},\{\mu_j\}) \,=\, \sum_{j=1}^{k} \sum_{x \in C_j} \lVert x - \mu_j \rVert_2^2.
\]

By default, k-Means uses Euclidean distance and assumes clusters are roughly convex and isotropic in the original feature space.

## How k-Means Clustering Works

- Initialization: choose \(k\) initial centroids (e.g., random points or k-Means++). In k-Means++, the probability of picking a new center \(x\) is proportional to the squared distance to the nearest already chosen center: \(p(x) \propto D(x)^2\).
- Assignment step: assign each point to its nearest centroid
  \[
  r_{ij} \,=\, \begin{cases}
  1 & \text{if } j = \arg\min_{\ell \in \{1,\dots,k\}} \lVert x_i - \mu_{\ell} \rVert_2^2, \\
  0 & \text{otherwise.}
  \end{cases}
  \]
- Update step: recompute each centroid as the mean of its assigned points
  \[
  \mu_j \,=\, \frac{1}{|C_j|} \sum_{x_i \in C_j} x_i.
  \]
- Repeat assignment and update until convergence (assignments stop changing or the objective decreases below a tolerance).

Stopping criteria commonly use a maximum number of iterations and/or a relative tolerance on \(J\).

## Applications

- Customer/user segmentation and cohort discovery
- Image compression and color quantization
- Vector quantization for signal processing
- Document/topic pre-clustering and feature hashing warm starts
- Anomaly detection via distance-to-centroid thresholds
- Initial seeds for more complex models (e.g., GMMs)

## Advantages and Disadvantages

Advantages

- Simple, fast, and widely available
- Scales linearly with number of samples and features (for the basic algorithm)
- Easy to interpret; centroids are means in feature space

Disadvantages

- Requires choosing \(k\)
- Sensitive to initialization and outliers
- Prefers spherical clusters under Euclidean distance; struggles with non-convex shapes
- Distance-based; features must be on comparable scales (standardization helps)

## Brief Computational Complexity

Let \(n\) be samples, \(d\) features, \(k\) clusters, and \(I\) iterations.

- Classic Lloyd’s algorithm: \(\mathcal{O}(nkdI)\)
- k-Means++ initialization: adds \(\mathcal{O}(nk)\) expected work upfront, improves convergence and quality
- Mini-batch k-Means (approximate): per-iteration \(\mathcal{O}(bkd)\) with batch size \(b \ll n\)
- Elkan/Hamerly/Yinyang accelerations (exact): fewer distance computations via triangle inequality; worst case \(\mathcal{O}(nkdI)\), often much faster in practice
- Kernel k-Means: uses a kernel matrix with \(\mathcal{O}(n^2)\) storage and typically superlinear time per iteration; impractical for very large \(n\)
- Distributed (e.g., H2O): per-iteration cost scales roughly with data per worker; ideal speedup approaches dividing work across nodes, subject to communication overhead

## The Differences Between Implementations

- k-Means (Classic/Lloyd)
  - In-memory, Euclidean distance, iterative assignment/mean updates.
  - Strong baseline for small/medium datasets.

- k-Means (H2O)
  - Distributed implementation in H2O-3. Key traits: data-parallel scalability across a cluster, support for common initializations (e.g., random, k-Means++-style), standard convergence controls (max iterations, tolerance), seeding for reproducibility, and preprocessing options (e.g., standardization).
  - Primarily Euclidean distance on numeric columns; intended for large datasets that do not fit on a single machine.

- k-Means (Kernel)
  - Replaces Euclidean distance in the original space with distances in an implicit feature space via a positive semi-definite kernel \(K(x, x')\). This enables non-linear, non-convex cluster boundaries at the cost of \(\mathcal{O}(n^2)\) memory/time.
  - With a kernel map \(\phi\) and cluster mean \(m_j = |C_j|^{-1}\sum_{x\in C_j} \phi(x)\), distances can be computed without explicit \(\phi\):
    \[
    \lVert \phi(x) - m_j \rVert_2^2 \,=\, K(x,x) - \frac{2}{|C_j|} \sum_{x_i\in C_j} K(x,x_i) + \frac{1}{|C_j|^2} \sum_{x_p\in C_j}\sum_{x_q\in C_j} K(x_p,x_q).
    \]

- k-Means (Fast)
  - Umbrella for speedups.
  - Exact accelerations (Elkan, Hamerly, Yinyang): prune distance checks using triangle inequality and centroid bounds; identical results to Lloyd given same init.
  - Approximate/online (Mini-batch, streaming): update centroids using small random batches or streams for near-linear scaling; quality may be slightly lower but often acceptable.

## How to Choose the Right k-Means Implementation

- Small to medium data (fits memory, modest \(n\)): Classic or Exact-accelerated (Elkan/Hamerly). Prefer k-Means++ init.
- Very large data or distributed environment: H2O’s k-Means for cluster-scale training; alternatively mini-batch on a single machine.
- Non-linear cluster shapes: Kernel k-Means with an appropriate kernel (e.g., RBF); be mindful of \(\mathcal{O}(n^2)\) memory/time.
- Streaming or time-constrained: Mini-batch k-Means; tune batch size \(b\) and iterations.
- High \(k\) or high \(d\): use dimensionality reduction (PCA) and k-Means++; exact accelerations shine with Euclidean distance and lower effective dimension.

Practical tips

- Always scale features (e.g., standardize) for Euclidean distance.
- Use multiple random restarts or k-Means++ to reduce bad local minima.
- Monitor inertia decrease and early-stop when improvements plateau.

## How to Evaluate k-Means Clustering

Distance metric

- Standard k-Means uses Euclidean distance. If cosine similarity is desired, consider spherical k-Means (normalize vectors) or use cosine distance within a variant designed for it.
- Ensure features are on comparable scales; otherwise distance is dominated by large-variance features.

Internal quality metrics (no ground truth)

- Inertia (within-cluster SSE):
  \[
  W_k \,=\, \sum_{j=1}^k \sum_{x \in C_j} \lVert x - \mu_j \rVert_2^2.
  \]
  Lower is better, but it always decreases as \(k\) increases; use in elbow plots.
- Silhouette score for a point \(x_i\):
  \[
  a(i) \,=\, \frac{1}{|C_{c(i)}|-1} \sum_{x\in C_{c(i)},\,x\neq x_i} d(x_i,x),\quad
  b(i) \,=\, \min_{\ell\neq c(i)} \frac{1}{|C_{\ell}|} \sum_{x\in C_{\ell}} d(x_i,x),\quad
  s(i) \,=\, \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}.
  \]
  The average \(\bar s\) over all points lies in \([-1,1]\); higher is better.
- Calinski–Harabasz and Davies–Bouldin indices: summarize separation vs. compactness; higher (CH) and lower (DB) are better, respectively.

External metrics (with labels)

- Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and Fowlkes–Mallows compare clusterings to ground truth labels.

Choosing \(k\)

- Elbow method on \(W_k\); look for diminishing returns.
- Average silhouette vs. \(k\); pick the \(k\) maximizing it.
- Gap statistic comparing \(W_k\) to a null reference distribution.

Kernel-specific evaluation

- Kernel choice and hyperparameters (e.g., RBF \(\gamma\)) strongly affect structure; select via grid search and internal metrics, balancing quality vs. \(\mathcal{O}(n^2)\) cost.

## Conclusion

k-Means remains a go-to clustering method because it is simple, fast, and effective for many problems. For small to medium data on a single machine, classic or accelerated k-Means with k-Means++ is a solid choice. For massive datasets or distributed environments, H2O’s implementation scales out. When cluster shapes are non-linear, kernel k-Means can capture complex boundaries at a significant computational cost. If time or memory is tight, mini-batch or other fast variants provide practical speed–quality trade-offs. Evaluate choices with scaling, robust initialization, and metrics like inertia and silhouette, and validate \(k\) using elbow, silhouette, or gap statistics.
